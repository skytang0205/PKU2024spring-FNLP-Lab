{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7344b209-30a4-47f6-92e4-3edd7daa473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'r', 'a', 'g', 'u', 'e', '</s>'] 107259\n"
     ]
    }
   ],
   "source": [
    "with open('bpe-training-data.txt', 'r') as file:\n",
    "    # 读取文件内容\n",
    "    data = file.read()\n",
    "data = data.split()\n",
    "\n",
    "data = [list(list(word) + ['</s>']) for word in data]\n",
    "\n",
    "print(data[0],len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a257996f-00ce-4c39-ad7b-a431f8c3e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "for word in data:\n",
    "    for letter in word:\n",
    "        if letter not in vocabulary:\n",
    "            vocabulary[letter] = 0\n",
    "        vocabulary[letter] += 1\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38739d5d-2359-40fa-b5fd-c1369881702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 b y</s> 728\n",
      "200 ti m 330\n",
      "300 u t</s> 192\n",
      "400 u e</s> 145\n",
      "500 a st 113\n",
      "600 e w</s> 90\n",
      "700 tak e</s> 75\n",
      "800 ex am 65\n",
      "900 E n 56\n",
      "1000 s ay 50\n",
      "1100 c a 44\n",
      "1200 re present 41\n",
      "1300 in fl 37\n",
      "1400 necess ary</s> 34\n",
      "1500 ab ility</s> 31\n",
      "1600 st ri 29\n",
      "1700 add ed</s> 27\n",
      "1800 ag e.</s> 25\n",
      "1900 d en 24\n",
      "2000 M c 23\n",
      "2100 tr y</s> 21\n",
      "2200 m ight</s> 20\n",
      "2300 A n</s> 19\n",
      "2400 pres ent</s> 18\n",
      "2500 sion .</s> 17\n",
      "2600 H all 17\n",
      "2700 il ,</s> 16\n",
      "2800 ag ency</s> 15\n",
      "2900 represent atives</s> 14\n",
      "3000 F or 14\n",
      "3100 C .</s> 13\n",
      "3200 C i 13\n",
      "3300 ep h 12\n",
      "3400 S e 12\n",
      "3500 on es,</s> 11\n",
      "3600 9 .</s> 11\n",
      "3700 tim ate</s> 11\n",
      "3800 sof tw 10\n",
      "3900 th anks</s> 10\n",
      "4000 hel ped</s> 10\n",
      "4100 we 're</s> 9\n",
      "4200 tal k</s> 9\n",
      "4300 concer n</s> 9\n",
      "4400 col li 9\n",
      "4500 suit able</s> 8\n",
      "4600 assu me</s> 8\n",
      "4700 win ning</s> 8\n",
      "4800 commit ted</s> 8\n",
      "4900 ish es</s> 7\n",
      "5000 pa th</s> 7\n",
      "5100 Lond on,</s> 7\n",
      "5200 scor ed</s> 7\n",
      "5300 week end,</s> 7\n",
      "5400 kilom et 7\n",
      "5500 ic es.</s> 6\n",
      "5600 c ity.</s> 6\n",
      "5700 ic i 6\n",
      "5800 rup tion</s> 6\n",
      "5900 re d,</s> 6\n",
      "6000 m ass 6\n",
      "6100 ex port</s> 6\n",
      "6200 Mr s.</s> 6\n",
      "6300 oper ation.</s> 5\n",
      "6400 Ol mer 5\n",
      "6500 lik es</s> 5\n",
      "6600 ab ducted</s> 5\n",
      "6700 Mean while,</s> 5\n",
      "6800 l am 5\n",
      "6900 di alog 5\n",
      "7000 inst antly</s> 5\n",
      "7100 H L 5\n",
      "7200 Nu eva</s> 5\n",
      "7300 B ased</s> 4\n",
      "7400 psycholog ist</s> 4\n",
      "7500 P P 4\n",
      "7600 H S 4\n",
      "7700 alleg ations</s> 4\n",
      "7800 care er</s> 4\n",
      "7900 flamenc o,</s> 4\n",
      "8000 environ ment.</s> 4\n",
      "8100 A verage</s> 4\n",
      "8200 w ounded</s> 4\n",
      "8300 Vlad im 4\n",
      "8400 revol ution 4\n",
      "8500 comp ul 4\n",
      "8600 u el 4\n",
      "8700 motorcycl es,</s> 4\n",
      "8800 refri ger 3\n",
      "8900 publ ish</s> 3\n",
      "9000 phys ics</s> 3\n",
      "9100 they 're</s> 3\n",
      "9200 cir cu 3\n",
      "9300 resul ted</s> 3\n",
      "9400 Tal e 3\n",
      "9500 2 9</s> 3\n",
      "9600 civ ic</s> 3\n",
      "9700 Leonard o</s> 3\n",
      "9800 cam pus 3\n",
      "9900 gu est</s> 3\n",
      "10000 det ect</s> 3\n",
      "10100 end or 3\n",
      "10200 Goo g 3\n",
      "10300 Stoib er</s> 3\n",
      "10400 capit al.</s> 3\n",
      "10500 pris on.</s> 3\n",
      "10600 fl ed</s> 3\n",
      "10700 pro spec 3\n",
      "10800 in dign 3\n",
      "10900 anc est 3\n",
      "11000 Trov it,</s> 3\n",
      "11100 Fran k,</s> 2\n",
      "11200 Bet ter</s> 2\n",
      "11300 ol ine</s> 2\n",
      "11400 occup i 2\n",
      "11500 ar t.</s> 2\n",
      "11600 forese eable</s> 2\n",
      "11700 lingu istic</s> 2\n",
      "11800 fil ing</s> 2\n",
      "11900 en ables</s> 2\n",
      "12000 h .</s> 2\n",
      "12100 clean er</s> 2\n",
      "12200 jum ping</s> 2\n",
      "12300 ris on</s> 2\n",
      "12400 emerg ed.</s> 2\n",
      "12500 arti st.</s> 2\n",
      "12600 M el 2\n",
      "12700 enti ve</s> 2\n",
      "12800 onn a</s> 2\n",
      "12900 ad ol 2\n",
      "13000 exp anded</s> 2\n",
      "13100 semi- final</s> 2\n",
      "13200 Or wel 2\n",
      "13300 c ow 2\n",
      "13400 differ ent.</s> 2\n",
      "13500 gene tic</s> 2\n",
      "13600 foo tw 2\n",
      "13700 pict ure 2\n",
      "13800 A tt 2\n",
      "13900 nan ot 2\n",
      "14000 outskir ts</s> 2\n",
      "14100 garb age.</s> 2\n",
      "14200 ess enti 2\n",
      "14300 increas ed.</s> 2\n",
      "14400 cigar ett 2\n",
      "14500 Dec .</s> 2\n",
      "14600 Sal vad 2\n",
      "14700 distric ts</s> 2\n",
      "14800 proc es 2\n",
      "14900 l us 2\n",
      "15000 Laetic ia,</s> 2\n",
      "15100 affec ted,</s> 2\n",
      "15200 war \"</s> 2\n",
      "15300 c roc 2\n",
      "15400 Vf B</s> 2\n",
      "15500 imagin ary</s> 2\n",
      "15600 op ted</s> 2\n",
      "15700 C oo 2\n",
      "15800 S oler 2\n",
      "15900 Tro pic 2\n"
     ]
    }
   ],
   "source": [
    "encode_list=[]\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    count += 1\n",
    "    bigram_dict = {}\n",
    "    for word in data:\n",
    "        for i in range(len(word)-1):\n",
    "            if (word[i],word[i+1]) not in bigram_dict:\n",
    "                bigram_dict[(word[i],word[i+1])] = 0\n",
    "            bigram_dict[(word[i],word[i+1])] += 1\n",
    "    w1,w2 = max(bigram_dict, key=bigram_dict.get)\n",
    "    if bigram_dict[(w1,w2)]==1: break\n",
    "    if count%100 == 0:\n",
    "        print(count, w1,w2,bigram_dict[(w1,w2)])\n",
    "    encode_list.append((w1,w2))\n",
    "    k=bigram_dict[(w1,w2)]\n",
    "    vocabulary[w1] -= k\n",
    "    vocabulary[w2] -= k\n",
    "    vocabulary[w1+w2] = k\n",
    "    \n",
    "    for word in data:\n",
    "        i = 0\n",
    "        while i < len(word) - 1:\n",
    "            if word[i] == w1 and word[i+1] == w2:\n",
    "                word[i:i+2] = [w1+w2]\n",
    "            i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ed6d4f-ea75-4b0c-9632-a6d28f760b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the type vocabulary: 16050\n",
      "Length of the training corpus: 124695\n"
     ]
    }
   ],
   "source": [
    "#token_dict = {k: v for k, v in token_dict.items() if v != 0}\n",
    "data = [letter for word in data for letter in word]\n",
    "print(\"size of the type vocabulary: {}\\nLength of the training corpus: {}\".format(len(vocabulary),len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de01e861-79c3-4d9c-8596-a7fe9645e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encode_bigram.txt', 'w') as file:\n",
    "    # 读取文件内容\n",
    "    for w1,w2 in encode_list:\n",
    "        file.write(w1+' '+w2+'\\n')\n",
    "\n",
    "\n",
    "with open('vocabulary.txt', 'w') as file:\n",
    "    # 读取文件内容\n",
    "    for w1 in vocabulary:\n",
    "        file.write(w1+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c140c2-f8f0-450f-ae32-02d6149f220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1385\n",
      "8010\n",
      "4796\n",
      "4200\n",
      "3926\n",
      "3766\n",
      "3634\n",
      "3477\n",
      "3392\n",
      "3340\n",
      "3261\n",
      "3189\n",
      "3140\n",
      "3105\n",
      "3049\n",
      "3017\n",
      "2988\n",
      "2946\n",
      "2917\n",
      "2898\n",
      "2878\n",
      "2861\n",
      "2838\n",
      "2820\n",
      "2811\n",
      "2795\n",
      "2775\n",
      "2744\n",
      "2727\n",
      "2706\n",
      "2693\n",
      "2674\n",
      "2663\n",
      "2646\n",
      "2634\n",
      "2609\n",
      "2602\n",
      "2593\n",
      "2579\n",
      "2574\n",
      "2570\n",
      "2560\n",
      "2552\n",
      "2538\n",
      "2526\n",
      "2515\n",
      "2508\n",
      "2502\n",
      "2497\n",
      "2488\n",
      "2485\n",
      "2478\n",
      "2461\n",
      "2428\n",
      "2420\n",
      "2418\n",
      "2416\n",
      "2414\n",
      "2410\n",
      "2403\n",
      "2403\n",
      "2398\n",
      "2391\n",
      "2388\n",
      "2385\n",
      "2380\n",
      "2375\n",
      "2362\n",
      "2350\n",
      "2327\n",
      "2324\n",
      "2319\n",
      "2313\n",
      "2311\n",
      "2308\n",
      "2301\n",
      "2298\n",
      "2293\n",
      "2291\n",
      "2285\n",
      "2278\n",
      "2270\n",
      "2261\n",
      "2258\n",
      "2255\n",
      "2252\n",
      "2249\n",
      "2248\n",
      "2245\n",
      "2243\n",
      "2241\n",
      "2238\n",
      "2234\n",
      "2230\n",
      "2221\n",
      "2216\n",
      "2208\n",
      "2206\n",
      "2196\n",
      "2192\n",
      "2189\n",
      "2180\n",
      "2174\n",
      "2173\n",
      "2171\n",
      "2168\n",
      "2166\n",
      "2166\n",
      "2165\n",
      "2165\n",
      "2164\n",
      "2164\n",
      "2164\n",
      "2164\n",
      "2162\n",
      "2161\n",
      "2160\n",
      "2158\n",
      "2157\n",
      "2156\n",
      "2155\n",
      "2155\n",
      "2153\n",
      "2152\n",
      "2151\n",
      "2144\n",
      "2142\n",
      "2140\n",
      "2138\n",
      "2134\n",
      "2134\n",
      "2127\n",
      "2127\n",
      "2126\n",
      "2121\n",
      "2121\n",
      "2117\n",
      "2117\n",
      "2117\n",
      "2116\n",
      "2115\n",
      "2115\n",
      "2113\n",
      "2112\n",
      "2105\n",
      "2103\n",
      "2101\n",
      "2101\n",
      "2097\n",
      "2096\n",
      "2096\n",
      "2095\n",
      "2094\n",
      "2094\n",
      "2094\n",
      "2094\n",
      "2093\n",
      "2093\n",
      "2092\n",
      "2092\n",
      "2092\n",
      "the final length of the article: 2092\n",
      "the token not appear in vocabulary: ['閳', 'ワ', '拷', '閳', 'ワ', '拷', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '凡', '閳', 'ワ', '拷', '閳', 'ユ', '獨', '閳', 'ユ', '敎', '閳', 'ユ', '獙', '閳', 'ユ', '獔', '閳', 'ユ', '獙', '閳', 'ユ', '攣', '閳', 'ユ', '獨', '閳', 'ユ', '笜', '閳', 'ワ', '拷', '閳', 'ユ', '獨', '閳', 'ユ', '獓', '閳', 'ユ', '笓', '閳', 'ワ', '拷', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '翻', '閳', 'ワ', '拷', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '獨', '閳', 'ユ', '发', '閳', 'ワ', '拷', '閳', 'ユ', '窔', '閳', 'ユ', '獡', '閳', 'ユ', '獟', '閳', 'ワ', '拷']\n",
      "size of the unk:105\n"
     ]
    }
   ],
   "source": [
    "with open('bpe-testing-article.txt', 'r') as file:\n",
    "    # 读取文件内容\n",
    "    test_data = file.read()\n",
    "\n",
    "test_data = test_data.split()\n",
    "\n",
    "test_data = [list(word) + ['</s>'] for word in test_data]\n",
    "\n",
    "print(len(test_data))\n",
    "\n",
    "test_data = [letter for word in test_data for letter in word]\n",
    "\n",
    "print(len(test_data))\n",
    "\n",
    "count = 0\n",
    "for bigram in encode_list:\n",
    "   # if len(bigram)!=2:print(bigram,count)\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(len(test_data))\n",
    "    i = 0\n",
    "    while i < (len(test_data) - 1):\n",
    "        if(i)==len(test_data):print(len(test_data),i)\n",
    "        if test_data[i] == bigram[0] and test_data[i+1] == bigram[1]:\n",
    "            test_data[i:i+2] = [bigram[0]+bigram[1]]\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "unk=[]\n",
    "for i in range(len(test_data)):\n",
    "    if test_data[i] not in vocabulary:\n",
    "        unk.append(test_data[i])\n",
    "        test_data[i]=\"<unk>\"\n",
    "\n",
    "print(\"the final length of the article: {}\\nthe token not appear in vocabulary: {}\\nsize of the unk:{}\".format(len(test_data),unk,len(unk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea0429a1-75ed-4b91-acc5-e4756b041b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['William', 'Whit', 'wor', 'th,', 'who', 'worked', 'as', 'a', 'writer', 'and', 'editor', 'at', 'The', 'New', 'Yor', 'ker', 'for', 'fourteen', 'years', 'and', 'then', 'ser', 'ved', 'as', 'editor-in-', 'chief', 'of', 'The', 'Atl', 'an', 'tic', 'Mon', 'thly', 'from', '198', '0', 'to', '1999,', 'died', 'last', 'Friday', 'in', 'Ar', 'k', 'ans', 'as,', 'the', 'state', 'where', 'he', 'was', 'bor', 'n', '.', 'He', 'was', 'a', 'brilli', 'ant', 'and', 'int', 'u', 'i', 'tive', 'editor', 'who', 'could', 'see', 'around', 'corners', 'and', 'beyond', 'writ', 'ers', '<unk>', '<unk>', '<unk>', '', 'horiz', 'ons', 'and', 'deep', 'into', 'thor', 'ny', 'man', 'us', 'crip', 'ts.', 'Every', 'one', 'who', 'worked', 'with', 'him', 'will', 'also', 'tell', 'you', 'that', 'he', 'was', 'a', 'pr', 'ince', 'of', 'a', 'f', 'ell', 'ow', '.', 'Th', 'rough', 'out', 'publishing', 'you', 'could', 'not', 'find', 'anybody', 'more', 'bel', 'ov', 'ed.', 'In', 'New', 'York,', 'edit', 'ors', 'who', 'come', 'from', 'L', 'it', 'tle', 'R', 'ock', 'are', 'rar', 'e,', 'even', 'more', 'so', 'ones', 'who', 'attended', 'the', 'University', 'of', 'Ok', 'la', 'hom', 'a', 'and', 'played', 'tr', 'um', 'pe', 't', 'in', 'the', 'So', 'on', 'ers', '<unk>', '<unk>', '<unk>', '', 'marching', 'b', 'and', '.', 'Bill', '(', 'as', 'colleagues', 'and', 'friends', 'called', 'him', ')', 'also', 'had', 'his', 'own', 'j', 'az', 'z', 'or', 'ch', 'est', 'ra', '.', 'He', 'played', 'for', 'so', 'many', 'd', 'ances', 'and', 'parties', 'that', 'he', 'covered', 'his', 'tu', 'ition', 'and', 'expenses', 'and', 'came', 'out', 'of', 'college', 'in', 'the', 'black', '.', 'On', 'c', 'e,', 'he', 'and', 'his', 'friend', 'from', 'L', 'it', 'tle', 'R', 'ock', 'Central', 'High', 'School', 'Charles', 'Patrick', '(P', 'at', ')', 'C', 'row', 'made', 'a', 'trip', 'to', 'St', '.', 'Lou', 'is', 'to', 'see', 'D', 'iz', 'zy', 'G', 'ill', 'esp', 'i', 'e.', 'After', 'their', 'third', 'night', 'in', 'the', 'front', 'row', ',', 'G', 'ill', 'esp', 'i', 'e', 'noticed', 'them,', 'asked', 'them', 'to', 'stay', 'after,', 'and', 'got', 'to', 'know', 'them.', 'Whit', 'worth', 'invited', 'him', 'to', 'perform', 'in', 'L', 'it', 'tle', 'R', 'ock', ';', 'G', 'ill', 'esp', 'i', 'e', 'accepted', 'and', 'st', 'ayed', 'with', 'him', 'and', 'his', 'mo', 'ther.', 'Later,', 'G', 'ill', 'esp', 'i', 'e', 'sometimes', 'st', 'ayed', 'in', 'Whit', 'wor', 'th', '<unk>', '<unk>', '<unk>', '', 'apartment', 'in', 'New', 'Yor', 'k.', 'Whit', 'wor', 'th', '<unk>', '<unk>', '<unk>', '', 'love', 'of', 'j', 'az', 'z', 'enh', 'anced', 'his', 'ed', 'iting', 'sk', 'ills', 'and', 'gave', 'him', 'refuge', 'and', 'res', 'ili', 'ence', 'and', 'a', 'different', 'way', 'to', 'thin', 'k.', 'After', 'colleg', 'e,', 'he', 'got', 'a', 'job', 'at', 'the', 'Ar', 'k', 'ans', 'as', 'G', 'az', 'ett', 'e,', 'where', 'he', 'covered', 'small', 'and', 'large', 'st', 'ories,', 'including', 'the', 'integr', 'ation', 'strug', 'gl', 'es', 'of', 'the', 'early', 'six', 'ties.', 'He', 'later', 'said', 'that', 'the', 'edit', 'ors', 'at', 'the', 'G', 'az', 'et', 'te', 'taught', 'him', 'how', 'to', 'write', 'a', 'news', 'stor', 'y.', 'His', 'friend', 'and', 'G', 'az', 'et', 'te', 'col', 'league', 'Charles', '(', 'Bud', 'd', 'y)', 'Por', 'ti', 's,', 'on', 'his', 'way', 'to', 'becoming', 'one', 'of', 'the', 'greatest', 'American', 'writers,', 'went', 'to', 'New', 'York', 'and', 'worked', 'at', 'the', 'Her', 'ald', 'Tri', 'bun', 'e.', 'When', 'the', 'Tri', 'bun', 'e', 'made', 'Por', 'tis', 'its', 'London', 'correspond', 'ent,', 'he', 'suggested', 'Whit', 'worth', 'as', 'his', 'replac', 'ement', 'in', 'New', 'Yor', 'k.', 'Whit', 'worth', 'moved', 'to', 'the', 'city,', 'and', 'after', 'a', 'few', 'years', 'of', 'covering', 'news', 'and', 'writing', 'features', 'for', 'the', 'Tri', 'bun', 'e', 'he', 'got', 'job', 'offers', 'from', 'both', 'The', 'New', 'Yor', 'ker', 'and', 'the', 'Times.', '(', 'Following', 'a', 'some', 'what', 'similar', 'rout', 'e,', 'P', 'at', 'C', 'row', 'also', 'ended', 'up', 'at', 'The', 'New', 'Yor', 'ker', '.)', 'In', 'that', 'era,', 'the', 'magazine', 'was', 'run', 'by', 'William', 'Sha', 'w', 'n', '.', 'Like', 'Whit', 'wor', 'th,', 'Sha', 'w', 'n', 'was', 'a', 'mus', 'ic', 'ian', '(', 'pi', 'ano', ').', 'As', 'a', 'magazine', 'edit', 'or,', 'he', 'edged', 'into', 'm', 'ys', 'tical', 'gen', 'i', 'us', 'territory.', 'Whit', 'worth', 'ad', 'mi', 'red', 'and', 'loved', 'Sha', 'w', 'n,', 'and', 'found', 'him', 'end', 'lessly', 'fascin', 'ating.', 'Out', 'side', 'of', 'famil', 'y,', 'Sha', 'w', 'n', 'was', 'the', 'most', 'important', 'person', 'in', 'Whit', 'wor', 'th', '<unk>', '<unk>', '<unk>', '', 'life.', 'For', 'all', 'Sha', 'w', 'n', '<unk>', '<unk>', '<unk>', '', 'diff', 'id', 'ence', 'and', 'quiet', 'ness,', 'he', 'sometimes', 'did', 'off-', 'the-', 'w', 'all', 'things.', 'Whit', 'worth', 'saved', 'a', 'few', 'gal', 'ley', 'pro', 'of', 's', 'with', 'notable', 'Sha', 'w', 'n', 'comments', 'on', 'them,', 'in', 'Sha', 'w', 'n', '<unk>', '<unk>', '<unk>', '', 'sm', 'all,', 'clear', 'hand', 'writ', 'ing.', 'Paul', 'ine', 'Ka', 'el,', 'the', 'movi', 'e', 'review', 'er,', 'was', 'writing', 'like', 'someone', 'poss', 'essed', 'in', 'the', 'mid-', 's', 'ev', 'enti', 'es,', 'and', 'Whit', 'worth', 'ed', 'ited', 'her.', 'Sha', 'w', 'n', 'saw', 'all', 'pro', 'of', 's.', 'Once', 'Ka', 'el', 'wrote', 'that', 'a', 'particular', 'actress', 'was', 'so', 'sex', 'y', 'that', 'she', 'was', 'like', '[', 'un', 'print', 'able', 'simil', 'e', ']', '.', 'When', 'the', 'proof', 'came', 'back', 'to', 'Whit', 'wor', 'th,', 'Sha', 'w', 'n', 'had', 'under', 'lined', 'the', 'sim', 'ile', 'and', 'written', 'in', 'the', 'marg', 'in,', '<unk>', '<unk>', '<unk>', 'h', 'y', 'does', 'she', 'do', 'this', '?', 'Wh', 'y?', 'Wh', 'y', '?', '<unk>', '<unk>', '<unk>', '', 'Dec', 'ades', 'later,', 'Bill', 'showed', 'me', 'that', 'saved', 'gal', 'ley', 'page', 'with', 'Sha', 'w', 'n', '<unk>', '<unk>', '<unk>', '', 'qu', 'er', 'y,', 'and', 'it', 'made', 'him', 'l', 'aug', 'h', 'all', 'over', 'again.', 'I', 'first', 'knew', 'about', 'Whit', 'worth', 'because', 'of', 'his', 'writ', 'ing.', 'The', 'summer', 'I', 'gradu', 'ated', 'from', 'college', '(', 'fifty-', 'one', 'years', 'ag', 'o', '),', 'I', 'read', 'a', 'short', 'New', 'Yor', 'ker', 'interview', 'with', 'J', 'on', 'a', 'than', 'W', 'inter', 's,', 'the', 'Oh', 'i', 'o-', 'born', 'com', 'edi', 'an.', 'With', 'inv', 'entive', 'sp', 'ell', 'ing,', 'the', 'piece', 'captured', 'how', 'W', 'inter', 's', 'pr', 'on', 'ounced', 'words', 'in', 'various', 'rural', 'acc', 'ent', 's', '<unk>', '<unk>', '<unk>', 'ou', '', 'know', ',', 'gu', 'ys', 'who', 'talk', 'la', 'h', 'k', '<unk>', '<unk>', '<unk>', 's', 's,', 'hold', '<unk>', '<unk>', '<unk>', 'r', 'mou', 'th', 'la', 'h', 'k', '<unk>', '<unk>', '<unk>', 's', 's.', 'An', 'Oh', 'i', 'o', 'an', 'my', 'self,', 'I', 'ad', 'mi', 'red', 'how', 'the', 'interview', 'er', 'had', 'got', 'W', 'inter', 's', 'down.', 'The', 'piece', 'was', 'in', 'The', 'Tal', 'k', 'of', 'the', 'T', 'own,', 'an', 'un', 'signed', 'department', 'back', 'th', 'en.', 'I', 'knew', 'somebody', 'who', 'wrote', 'for', 'the', 'department,', 'and', 'I', 'found', 'out', 'who', 'had', 'written', 'the', 'piec', 'e.', 'It', 'made', 'me', 'want', 'to', 'write', 'for', 'The', 'New', 'Yor', 'ker', '<unk>', '<unk>', '<unk>', 'n', 'd', 'even', 'think', 'I', 'cou', 'l', 'd.', 'Sha', 'w', 'n', 'later', 'h', 'ired', 'me', 'as', 'a', 'Tal', 'k', 'repor', 'ter,', 'and', 'I', 'read', 'all', 'the', 'Whit', 'worth', 'pieces', 'in', 'the', 'magaz', 'ine', '<unk>', '<unk>', '<unk>', '', 'libr', 'ary.', 'He', 'did', 'profiles', 'of', 'Col', 'onel', 'Sand', 'ers,', 'the', 'fri', 'ed-', 'ch', 'ick', 'en', 'fig', 'ure', 'head,', 'and', 'Roger', 'M', 'ill', 'er,', 'the', 'country', 'sing', 'er-', 'song', 'writer,', 'and', 'Jo', 'e', 'Frank', 'lin,', 'the', 'w', 'ack', 'y,', 'local', 'lat', 'e-', 'night', 'talk', '-', 'show', 'host', ',', 'and', 'Dav', 'e,', 'the', 'aut', 'o', 'grap', 'h', 'h', 'ound,', 'whose', 'al', 'way', 's-', 'at-', 'top-', 'volume', 'dialogue', 'Whit', 'worth', 'con', 've', 'yed', 'entirely', 'in', 'capital', 'let', 'ters.', 'He', 'did', 'a', 'for', '-', 'the', '-h', 'eck', '-of-', 'it,', '<unk>', '<unk>', '<unk>', 'o', 'news', 'h', 'oo', 'k', '<unk>', '<unk>', '<unk>', '', 'piece', 'about', 'a', 'p', 'ig', '', 'far', 'm', 'in', 'Pennsylvan', 'ia,', 'illustrated', 'with', 'a', 'photo', '(', 'one', 'of', 'the', 'first', '-', 'ever', 'edit', 'or', 'ial', 'photos', 'in', 'the', 'magaz', 'ine', ')', 'of', 'a', 'p', 'ig', 'let', 'd', 'ressed', 'in', 'a', 'ba', 'by', 'bon', 'net', 'and', 're', 'clin', 'ing', 'in', 'some', 'bod', 'y', '<unk>', '<unk>', '<unk>', '', 'arm', 's.', 'He', 'described', 'a', 'group', 'of', 'p', 'ig', 'lets', 'hop', 'ping', 'around', 'for', 'joy', 'and', 'said', 'that', 'if', 'they', '<unk>', '<unk>', '<unk>', '', 'been', 'on', 'a', 'hard', 'sur', 'face,', 'their', 'h', 'oo', 'ves', 'would', 'have', 's', 'ounded', '<unk>', '<unk>', '<unk>', 'ike', 'a', 'room', 'ful', 'of', 'expert', 'typ', 'ist', 's', '.', '<unk>', '<unk>', '<unk>', '', 'His', 'many', 'pieces', 'have', 'never', 'been', 'collected', 'and', 'published', 'as', 'a', 'book,', 'and', 'they', 'should', 'be.', 'Ev', 'ent', 'ually', 'he', 'accepted', 'Sha', 'w', 'n', '<unk>', '<unk>', '<unk>', '', 'offer', 'of', 'a', 'full-', 'time', 'ed', 'iting', 'position', 'and', 'worked', 'on', 'pieces', 'like', 'the', 'multi', 'part', 'ex', 'cer', 'p', 'ts', 'from', 'Robert', 'Car', 'o', '<unk>', '<unk>', '<unk>', '', 'book', '<unk>', '<unk>', '<unk>', 'he', 'Power', 'Bro', 'ker', ',', '<unk>', '<unk>', '<unk>', '', 'about', 'Robert', 'Mos', 'es.', 'In', '197', '9,', 'Whit', 'worth', 'ed', 'ited', 'my', 'first', 'signed', 'repor', 'ting', 'piec', 'e,', 'and', 'we', 'became', 'friend', 's.', 'When', 'people', 'started', 'wond', 'ering', 'when', 'Sha', 'w', 'n', 'would', 'reti', 're,', 'Whit', 'worth', 'seemed', 'the', 'natural', 'choice', 'to', 'succeed', 'him.', 'The', 'magaz', 'ine', '<unk>', '<unk>', '<unk>', '', 'then', 'chair', 'man,', 'Peter', 'F', 'le', 'is', 'ch', 'mann,', 'said', 'he', 'would', 'give', 'Whit', 'worth', 'the', 'job', '.', 'Ag', 'ain,', 'he', 'had', 'an', 'emb', 'arr', 'ass', 'ment', 'of', 'offer', 's.', 'At', 'about', 'the', 'same', 'time,', 'M', 'ort', 'Z', 'uc', 'ker', 'man,', 'a', 'real-estate', 'developer', 'who', 'had', 'bought', 'The', 'Atl', 'antic', ',', 'asked', 'him', 'to', 'take', 'over', 'that', 'magazine.', 'The', 'office', 'politics', 'were', 'complic', 'at', 'ed;', 'partly', 'to', 'avoid', 'being', 'used', 'as', 'a', 'l', 'ever', 'to', 'force', 'Sha', 'w', 'n', 'out,', 'Whit', 'worth', 'took', 'the', 'Atl', 'an', 'tic', 'job', 'and', 'moved', 'to', 'Bo', 'st', 'on.', 'At', 'The', 'Atl', 'antic', ',', 'he', 'ed', 'ited', 'a', 'dozen', 'of', 'my', 'pieces,', 'or', 'more.', 'He', 'meant', 'the', 'world', 'to', 'me.', 'Other', 'writers', 'he', 'worked', 'with', 'say', 'the', 'same.', 'He', 'had', 'a', 'manner', 'of', 'sus', 'pending', 'himself', 'att', 'enti', 'v', 'ely', 'which', 'brought', 'out', 'the', 'essenti', 'al,', 'best', 'version', 'of', 'y', 'our', 'self.', 'He', 'was', 'thin', ',', 'with', 'high', 't', 'em', 'ples', 'and', 'a', 'high', 'fore', 'head,', 'and', 'a', 'mis', 'chi', 'ev', 'ous,', 'anticip', 'atory', 'expression', 'in', 'his', 'ey', 'es.', 'After', 'M', 'ort', 'Z', 'uc', 'ker', 'man', 'sold', 'The', 'Atl', 'antic', ',', 'the', 'new', 'owner', 'brought', 'in', 'a', 'new', 'edit', 'or,', 'and', 'Bill', 'moved', 'back', 'to', 'L', 'it', 'tle', 'R', 'ock', '.', 'I', 'knew', 'and', 'liked', 'his', 'son,', 'Mat', 't,', 'who', 'worked', 'in', 'a', 'Ch', 'else', 'a', 'art', 'gall', 'er', 'y.', 'Matt', 'moved', 'to', 'Min', 'ne', 'ap', 'ol', 'is,', 'and', 'he', 'passed', 'away', 'two', 'years', 'ag', 'o', ';', 'B', 'ill', '<unk>', '<unk>', '<unk>', '', 'wife,', 'Car', 'ol', 'y', 'n,', 'had', 'died', 'many', 'years', 'before.', 'Bill', 'ed', 'ited', 'man', 'us', 'crip', 'ts', 'in', 'his', 'semi-', 'reti', 're', 'ment', 'and', 'applied', 'his', 'intense', 'dil', 'ig', 'ence', 'to', 'one', 'un', 'wi', 'el', 'dy', 'book', 'after', 'an', 'other', '.', 'The', 'pages', 'p', 'iled', 'up', 'in', 'orderly', 'st', 'acks', 'in', 'the', 'office', 'in', 'his', 'suburban', 'house.', 'He', 'came', 'back', 'to', 'New', 'York', 'for', 'a', 'book', 'party', 'for', 'one', 'of', 'his', 'edit', 'ees,', 'and', 'that', '<unk>', '<unk>', '<unk>', '', 'where', 'I', 'met', 'his', 'daugh', 'ter,', 'K', 'ather', 'ine', 'St', 'ew', 'ar', 't,', 'a', 'writer', 'and', 'editor', 'who', 'lives', 'in', 'L', 'it', 'tle', 'R', 'ock', 'and', 'who', 'took', 'care', 'of', 'him', 'during', 'his', 'final', 'health', 'problems.', 'Over', 'the', 'years,', 'I', 'dro', 've', 'out', 'to', 'visit', 'him', 'in', 'L', 'it', 'tle', 'R', 'ock', 'from', 'my', 'house', 'in', 'New', 'Jersey', 'three', 'or', 'four', 'times,', 'and', 'flew', 'out', 'once.', 'I', 'st', 'ayed', 'with', 'him,', 'and', 'we', 'went', 'out', 'to', 'Mexican', 'restaur', 'ants', 'and', 'watched', 'music', 'vide', 'os', 'on', 'his', 'wi', 'de', '-', 'screen', 'TV', 'and', 'list', 'ened', 'to', 'j', 'az', 'z', 'record', 'ings', 'in', 'his', 'living', 'room.', 'On', 'c', 'e,', 'he', 'put', 'on', 'an', 'L', 'P', 'with', 'a', 'solo', 'by', 'Lou', 'is', 'Ar', 'm', 'strong', '.', 'When', 'it', 'was', 'over,', 'Bill', 's', 'at', 'for', 'a', 'few', 'minutes,', 'quiet', 'ly', 'transport', 'ed,', 'and', 'then', 'said,', '<unk>', '<unk>', '<unk>', 'il', 'es', 'Dav', 'is', 'once', 'said', 'that,', 'before', 'Lou', 'is', 'Ar', 'm', 'str', 'ong,', 'all', 'American', 'music', 'was', 'cor', 'n', 'y.', '<unk>', '<unk>', '<unk>', '', 'The', 'statement', 'may', 'not', 'be', 'strictly', 'true,', 'and', 'Mil', 'es', 'Dav', 'is', 'may', 'not', 'have', 'said', 'it,', 'but', 'non', 'e', 'thel', 'ess', 'it', 'changed', 'my', 'percep', 'tion.', 'Stephen', 'F', 'ost', 'er', 'and', 'John', 'Philip', 'S', 'ous', 'a', 'will', 'never', 'sound', 'the', 'same', 'to', 'me', 'again.', 'After', 'one', 'vis', 'it,', 'I', 'was', 'about', 'to', 'leav', 'e,', 'having', 'gone', 'out', 'through', 'the', 'gar', 'age', 'and', 'put', 'my', 'suit', 'case', 'in', 'my', 'car,', 'which', 'was', 'in', 'the', 'driv', 'ew', 'ay.', 'The', 'gar', 'age', 'door', 'was', 'open,', 'and', 'we', 'were', 'standing', 'next', 'to', 'his', 'car.', 'I', 'said', 'good', 'by', 'e,', 'and', 'Bill', 'said,', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '', 'afraid', 'I', '<unk>', '<unk>', '<unk>', 'l', 'never', 'see', 'you', 'again', '.', '<unk>', '<unk>', '<unk>', '', 'I', 'said', 'he', 'would', 'see', 'me', 'again,', 'and', 'in', 'fact', 'I', 'did', 'return', 'to', 'L', 'it', 'tle', 'R', 'ock', 'the', 'next', 'year', 'or', 'the', 'year', 'af', 'ter.', 'But,', 'when', 'he', 'said', 'that,', 'I', 'suddenly', 'felt', 'such', 'love', 'for', 'him.', 'You', 'can', 'be', 'so', 'close', 'to', 'someone', 'and', 'not', 'really', 'understand', 'how', 'close', 'you', 'ar', 'e.', 'I', 'have', 'done', 'what', 'many', 'e', 'ul', 'og', 'ists', 'do', 'and', 'have', 'made', 'this', 'too', 'much', 'about', 'me,', 'but', 'the', 'number', 'of', 'his', 'admir', 'ers', 'is', 'legi', 'on.', 'The', 'fortun', 'ate', 'writers', 'he', 'ed', 'ited', 'during', 'his', 'career', 'were', 'each', 'another', 'aspect', 'of', 'him.', 'I', 'am', 'only', 'one', 'of', 'hundreds', 'who', 'loved', 'B', 'ill.']\n"
     ]
    }
   ],
   "source": [
    "print([text.replace(\"</s>\",'') for text in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f4d44-c810-42ab-909d-2c3f6e3e4fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
